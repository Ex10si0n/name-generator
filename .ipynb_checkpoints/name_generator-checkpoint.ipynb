{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN网络生成随机名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPYRIGHT BY EX10SI0N @ www.github.com/Ex10si0n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据采集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理读入文件为str类型\n",
    "source = open(\"namesrc.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理source中的unique\n",
    "source_set = sorted(set(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建从非重复字符到索引的映射\n",
    "char2idx = {u:i for i, u in enumerate(source_set)}\n",
    "idx2char = np.array(source_set)\n",
    "text_as_int = np.array([char2idx[c] for c in source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ' ' :   0,\n",
      "  '业' :   1,\n",
      "  '东' :   2,\n",
      "  '久' :   3,\n",
      "  '义' :   4,\n",
      "  '乐' :   5,\n",
      "  '云' :   6,\n",
      "  '亚' :   7,\n",
      "  '亦' :   8,\n",
      "  '京' :   9,\n",
      "  '亭' :  10,\n",
      "  '仁' :  11,\n",
      "  '仑' :  12,\n",
      "  '仟' :  13,\n",
      "  '仲' :  14,\n",
      "  '伟' :  15,\n",
      "  '伦' :  16,\n",
      "  '佐' :  17,\n",
      "  '佑' :  18,\n",
      "  '佳' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 索引表打印测试\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练样本和目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "华\n",
      "君\n",
      " \n",
      "子\n",
      "睿\n"
     ]
    }
   ],
   "source": [
    "seq_length = 2\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'华君 '\n",
      "'子睿 '\n",
      "'宏君 '\n",
      "'旦宇 '\n",
      "'甫宇 '\n"
     ]
    }
   ],
   "source": [
    "seq = char_dataset.batch(seq_length + 1, drop_remainder = True)\n",
    "for item in seq.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = seq.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '华君'\n",
      "Target data: '君 '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 49 ('华')\n",
      "  expected output: 57 ('君')\n",
      "Step    1\n",
      "  input: 57 ('君')\n",
      "  expected output: 0 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 2), (64, 2)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设定批和缓冲区大小\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = len(source_set)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(set_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(set_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(set_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  set_size = len(source_set),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2, 476) # (batch_size, sequence_length, set_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, set_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           121856    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 476)           487900    \n",
      "=================================================================\n",
      "Total params: 4,548,060\n",
      "Trainable params: 4,548,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([184, 365])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 2, 476)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       6.1655073\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 81 steps\n",
      "Epoch 1/10\n",
      "81/81 [==============================] - 5s 57ms/step - loss: 4.2491\n",
      "Epoch 2/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.6700\n",
      "Epoch 3/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.5742\n",
      "Epoch 4/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.4441\n",
      "Epoch 5/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.3324\n",
      "Epoch 6/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.2346\n",
      "Epoch 7/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.1630\n",
      "Epoch 8/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.1056\n",
      "Epoch 9/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.0549\n",
      "Epoch 10/10\n",
      "81/81 [==============================] - 4s 45ms/step - loss: 2.0214\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(set_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            121856    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 476)            487900    \n",
      "=================================================================\n",
      "Total params: 4,548,060\n",
      "Trainable params: 4,548,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  temperature = 3.0\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "name_arr = generate_text(model, start_string=u\"卓\")\n",
    "name_arr = name_arr.replace(\" \",\"\")\n",
    "interval = re.compile('.{2}')\n",
    "lastname = ' '.join(interval.findall(name_arr))\n",
    "lastname = lastname.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 姓氏处理（数据来自百家姓）\n",
    "概率模型: `p(x) = e^(-x * (4 / 97))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, csv\n",
    "surname = open(\"surnamesrc.txt\").read()\n",
    "surname = surname.split()\n",
    "p = [math.exp(- x * (4 / 97)) for x in range(1, len(surname))]\n",
    "row = random.random()\n",
    "surname_set = []\n",
    "for j in range(len(name_arr)):\n",
    "    for i in range(len(surname) - 1):\n",
    "        if p[i] <= row:\n",
    "            surname_set.append(surname[i])\n",
    "            row = random.random()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 385 names generated\n"
     ]
    }
   ],
   "source": [
    "n = min(len(surname), len(lastname))\n",
    "print(\"result: \" + str(n) + \" names generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['曾卓申', '赵鑫纪', '钱锦邦', '孙泓倬', '李胥韬', '周荣轩', '吴衡小', '郑君桦', '王诺箫', '冯斌跃', '陈奥亚', '褚彦宗', '卫涛森', '蒋斌允', '沈乐博', '韩然军', '杨骏炜', '朱彤进', '秦龙霖', '尤皓周', '许鸿蕾', '何旷度', '吕昕键', '施珝剑', '张意薇', '孔炜琛', '曹晏嘉', '严贤楠', '华牧阳', '金智俊', '魏豪智', '陶凡熠', '姜盛拓', '戚木睿', '谢宇皓', '邹堇辉', '喻皓尘', '柏斌润', '水贞辉', '窦熊思', '章勤盛', '云璟贞', '苏俞嵘', '潘悦斌', '葛林雨', '奚祖琛', '范飞坤', '彭科琛', '郎颜聚', '鲁濠郎', '韦烁斌', '昌云纹', '马豪澜', '苗烁旻', '凤慧林', '花基岩', '方盛元', '俞键瀚', '任云萌', '袁骏雅', '柳珏辰', '酆姬翔', '鲍祖生', '史政嵘', '唐京久', '费军继', '廉温勋', '岑云薇', '薛俊蕾', '雷焱淇', '贺云菲', '倪贤思', '汤伟雅', '滕孝诩', '殷珅均', '罗韦功', '毕桦盛', '郝文涛', '邬浩森', '安瑞宁', '常睿和', '乐朗君', '于涛生', '时基珏', '傅飘森', '皮珏双', '卞震壮', '齐晨林', '康珅圣', '伍泓郎', '余政棠', '元豪勋', '卜凡焱', '顾棠楚', '孟云轩', '平成绮', '黄颜俊', '和峰富', '穆竿衡', '萧剑剑', '尹岩嘉', '姚凡嘉', '邵尚珅', '湛彦诺', '汪庄文', '祁莱程', '毛亭嘉', '禹盛语', '狄瑜俊', '米谦侦', '贝轩朗', '明伟亭', '臧华洲', '计峰焱', '伏涉梓', '成然智', '戴白琛', '谈韦胥', '宋凡鸿', '茅彦泓', '庞嘉庄', '熊珩业', '纪娴莉', '舒辰涛', '屈祺翊', '项祯昕', '祝瑞智', '董纹钧', '梁超洲', '杜林箫', '阮闻宗', '蓝睿慕', '闵嘉誉', '席俞豪', '季盛兆', '麻子程', '强伦瑾', '贾昊曦', '路光轩', '娄韬俞', '危桦兴', '江项俞', '童斌业', '颜羽泓', '郭铮凯', '梅均鸣', '盛伟晓', '林鹤森', '刁铮彬', '徐韦军', '邱诺熙', '骆宁瀚', '高璟煊', '夏明君', '蔡嘉超', '田兴熊', '樊伦彦', '胡谨灏', '凌杰业', '霍淳祺', '虞涛怡', '万深翔', '支斌祺', '柯嘉永', '昝风诩', '管棉继', '卢钢烜', '莫宜彦', '经冬聪', '房樾勤', '裘瑞雨', '缪麒祺', '干鹏珝', '解梓聚', '应荫俊', '宗纹庄', '丁藏宇', '宣泓业', '邓灏天', '贲云俊', '郁子睿', '单诺盛', '杭聪格', '洪泓凯', '包晗依', '诸荫堇', '左飞祖', '石翊科', '崔德森', '吉佐樾', '钮乐功', '龚俞俊', '程东弘', '嵇希尊', '邢小慎', '滑焱鹤', '裴泓智', '陆度智', '荣睿贝', '翁宜景', '荀睿松', '羊祎勤', '於羿睿', '惠煜成', '甄弘薇', '麴标乐', '家力军', '封焱炜', '芮学佑', '羿跃学', '储姬豪', '靳骏正', '汲思业', '邴宝茂', '糜翊双', '松科迪', '段乐韦', '富景珂', '巫军麒', '乌瀚科', '焦凯权', '巴科莱', '弓乐辰', '牧纹尚', '隗成焱', '山辉岸', '谷震程', '关亦贤', '妫琛拂', '桂聪彦', '国贝曦', '海香泓', '侯卓颜', '后硕依', '弘璟凯', '庚飞轩', '巩莓祺', '贡剑韬', '古煊尊', '甘姣莱', '郜岳智', '戈舒羽', '艾楠跃', '文昌漾', '丰宗棠', '鄂纪佳', '法淇安', '都莱科', '钭磊迪', '堵林桥', '扶泓圩', '符双炜', '楚诺学', '从胥彦', '苍炜贤', '池成鹏', '仇雄键', '是松瀚', '党孝继', '东壑淇', '白薇轩', '班鹤瑞', '暴翎伟', '敖鸿金', '能榕正', '聂景铭', '牛曦楚', '农桐梁', '薄烁颜', '步棱韬', '边菲嘉', '晁鸿超', '巢堇明', '车政皓', '查洲熙', '柴康豪', '荆俊宇', '居双宪', '盖宇云', '蒯琮航', '夔宇政', '利烜俞', '连琮天', '廖易辉', '蔺旻嘉', '刘彤琛', '龙意容', '乜颜剑', '蒙嘉昆', '禄秀桂', '劳铠俞', '满颜蓝', '那礼力', '宓飞轩', '宁熠纪', '隆斌熙', '栾维智', '终桂郎', '仲彦韦', '阎皓峰', '晏凡景', '燕纹钰', '翟天韦', '欧凡桦', '竺昆仲', '訾俊林', '祖琛旻', '越简孝', '宰纹莱', '黎超颜', '厉誉飞', '郦深格', '景颜鸿', '鞠宝勋', '空香纹', '寇煊标', '匡轩昊', '赖羿基', '简俊鹤', '暨统贞', '冀烁昱', '郏嘉焱', '姬毅宇', '桓涛风', '益歌磊', '仉烁君', '仰贝韬', '养济朗', '伊歌达', '易维祺', '鱼威豫', '雍烨义', '卓斌贤', '钟瑾硕', '印超蓓', '叶桐声', '庄艺弈', '旷剑盛', '邝祺飞', '来彦贝', '操彤诺', '承小豪', '浮力霖', '洑陌慕', '蓬铠勤', '朋颜博', '朴政政', '詹盛悦', '游俊凡', '芈浩镇', '睦飞嵘', '冒昕瑜', '门歌睿', '慕仲亭', '谬深轩', '麦培德', '南飞望', '区誉成', '苟周桦', '光瑾钧', '刚炜福', '归泓杰', '宦杰政', '荤歌荣', '户楠瑞', '蹇文瑞', '翦梓键', '揭翎钧', '矫雨弘', '鹿盛谦', '楼昆堇', '励炜桥', '商山倩', '尚莓潇', '韶舒飞', '萨姬晗', '赛卓彦', '桑科韦', '沙熙先', '璩庄超', '茹政明', '戎斌意', '容朗仲', '融诩瓯', '曲熊熙']\n"
     ]
    }
   ],
   "source": [
    "name = []\n",
    "for i in range(n):\n",
    "    name.append(surname[i] + lastname[i])\n",
    "print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
